{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, sys, glob, datetime, yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import trange\n",
    "from einops import rearrange\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from ldm.util import instantiate_from_config\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_config(config, sd):\n",
    "    model = instantiate_from_config(config)\n",
    "    model.load_state_dict(sd,strict=False)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model(config, ckpt, gpu, eval_mode):\n",
    "    if ckpt:\n",
    "        print(f\"Loading model from {ckpt}\")\n",
    "        pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "        global_step = pl_sd[\"global_step\"]\n",
    "    else:\n",
    "        pl_sd = {\"state_dict\": None}\n",
    "        global_step = None\n",
    "    model = load_model_from_config(config.model,\n",
    "                                   pl_sd[\"state_dict\"])\n",
    "\n",
    "    return model, global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_to_np(x):\n",
    "    # saves the batch in adm style as in https://github.com/openai/guided-diffusion/blob/main/scripts/image_sample.py\n",
    "    sample = x.detach().cpu()\n",
    "    # sample = ((sample + 1) * 127.5).clamp(0, 255).to(torch.uint8)\n",
    "    # sample = sample.permute(0, 2, 3, 1)\n",
    "    sample = sample.contiguous()\n",
    "    return sample\n",
    "\n",
    "def custom_to_pil(x):\n",
    "    x = x.detach().cpu()\n",
    "    x = torch.clamp(x, -1., 1.)\n",
    "    x = (x + 1.) / 2.\n",
    "    x = x.permute(1, 2, 0).numpy()\n",
    "    x = (255 * x).astype(np.uint8)\n",
    "    x = Image.fromarray(x)\n",
    "    if not x.mode == \"RGB\":\n",
    "        x = x.convert(\"RGB\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_logs(logs,gt, path, n_saved=0, key=\"sample\", np_path=None):\n",
    "    for k in logs:\n",
    "        if k == key:\n",
    "            batch = logs[key]\n",
    "            if np_path is None:\n",
    "                for x in batch:\n",
    "                    img = custom_to_pil(x)\n",
    "                    imgpath = os.path.join(path, f\"{key}_{n_saved:06}.png\")\n",
    "                    img.save(imgpath)\n",
    "                    n_saved += 1\n",
    "            else:\n",
    "                npbatch = custom_to_np(batch)\n",
    "                shape_str = \"x\".join([str(x) for x in npbatch.shape])\n",
    "                nppath = os.path.join(np_path, f\"{n_saved}-{shape_str}-samples.npz\")\n",
    "                np.savez(nppath, npbatch,gt)\n",
    "                n_saved += npbatch.shape[0]\n",
    "    return n_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model,x):\n",
    "    log = dict()\n",
    "    t0 = time.time()\n",
    "    x = x.to(memory_format=torch.contiguous_format).float()\n",
    "    t1 = time.time()\n",
    "    x_sample, _ = model(x.to(\"cuda\"))\n",
    "    log[\"sample\"] = x_sample\n",
    "    log[\"time\"] = t1 - t0\n",
    "    # log['throughput'] = sample.shape[0] / (t1 - t0)\n",
    "    # print(f'Throughput for this batch: {log[\"throughput\"]}')\n",
    "    return log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, logdir, batch_size=50, vanilla=False, custom_steps=None, eta=None, n_samples=50000, nplog=None):\n",
    "\n",
    "    import ldm.data.ppg2abp as ppg2abp\n",
    "    dataset = ppg2abp.PPG2ABPDataset_v3_Test()\n",
    "    n_samples = len(dataset)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,shuffle=False)\n",
    "    # batch_size = 128\n",
    "    n_saved = 0\n",
    "    # path = logdir\n",
    "    all_images = []\n",
    "    all_gt_images = []\n",
    "    print(f\"Running conditional sampling for {n_samples} samples\")\n",
    "    for i in trange(n_samples // batch_size, desc=\"Sampling Batches (conditional)\"):\n",
    "        data = next(iter(train_loader))\n",
    "        input = data['gt_image']\n",
    "        # vae \n",
    "        logs = sample(model,input)\n",
    "        all_images.extend([custom_to_np(logs[\"sample\"])])\n",
    "        all_gt_images.extend([data['gt_image']])\n",
    "        n_saved = save_logs(logs,data['gt_image'], logdir, n_saved=n_saved, key=\"sample\",np_path=nplog)\n",
    "        if n_saved >= n_samples:\n",
    "            print(f'Finish after generating {n_saved} samples')\n",
    "            break\n",
    "    all_img = np.concatenate(all_images, axis=0)\n",
    "    all_img = all_img[:n_samples]\n",
    "    all_gt_img = np.concatenate(all_gt_images, axis=0)\n",
    "    all_gt_img = all_gt_img[:n_samples]\n",
    "    shape_str = \"x\".join([str(x) for x in all_img.shape])\n",
    "    nppath = os.path.join(nplog, f\"{shape_str}-samples.npz\")\n",
    "    np.savez(nppath, all_img,all_gt_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"-r\",\n",
    "        \"--resume\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        help=\"load from logdir or checkpoint in logdir\",\n",
    "        default=r\"./models/first_stage_models/v3_ppg2abp-kl-f4_2\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-n\",\n",
    "        \"--n_samples\",\n",
    "        type=int,\n",
    "        nargs=\"?\",\n",
    "        help=\"number of samples to draw\",\n",
    "        default=50000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-e\",\n",
    "        \"--eta\",\n",
    "        type=float,\n",
    "        nargs=\"?\",\n",
    "        help=\"eta for ddim sampling (0.0 yields deterministic sampling)\",\n",
    "        default=1.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-v\",\n",
    "        \"--vanilla_sample\",\n",
    "        default=False,\n",
    "        action='store_true',\n",
    "        help=\"vanilla sampling (default option is DDIM sampling)?\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-l\",\n",
    "        \"--logdir\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        help=\"extra logdir\",\n",
    "        default=\"none\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-c\",\n",
    "        \"--custom_steps\",\n",
    "        type=int,\n",
    "        nargs=\"?\",\n",
    "        help=\"number of steps for ddim and fastdpm sampling\",\n",
    "        default=50\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        type=int,\n",
    "        nargs=\"?\",\n",
    "        help=\"the bs\",\n",
    "        default=10\n",
    "    )\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config ['./models/first_stage_models/v3_ppg2abp-kl-f4_2\\\\abp.yaml']\n",
      "none ['./models/first_stage_models/v3_ppg2abp-kl-f4_2'] \\\n",
      "{'model': {'base_learning_rate': 4.5e-06, 'target': 'ldm.models.autoencoder1D_v1.AutoencoderKL', 'params': {'monitor': 'val/rec_loss', 'embed_dim': 3, 'image_key': 'gt_image', 'ddconfig': {'double_z': True, 'z_channels': 3, 'resolution': 256, 'in_channels': 1, 'out_ch': 1, 'ch': 128, 'ch_mult': [1, 2, 4], 'num_res_blocks': 2, 'attn_resolutions': [16], 'dropout': 0.0}, 'lossconfig': {'target': 'ldm.modules.losses.contperceptual.LPIPSWithDiscriminator_2', 'params': {'disc_start': 20001, 'kl_weight': 0.001, 'disc_weight': 0.5}}}}, 'data': {'target': 'main.DataModuleFromConfig', 'params': {'batch_size': 16, 'num_workers': 8, 'train': {'target': 'ldm.data.ppg2abp.PPG2ABPDataset_v3_Train', 'params': {'data_len': -1, 'size': 256}}, 'validation': {'target': 'ldm.data.ppg2abp.PPG2ABPDataset_v3_Val', 'params': {'size': 256}}}}, 'lightning': {'callbacks': {'image_logger': {'target': 'main.FigLogger', 'params': {'batch_frequency': 2000, 'max_images': 8, 'increase_log_steps': False}}}, 'trainer': {'gpus': '0,', 'benchmark': True, 'accumulate_grad_batches': 2}}, '--f': 'c:\\\\Users\\\\bsa\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v39049c092d5ef4b0ca8c737552ba904f32770353a.json'}\n",
      "Loading model from ./models/first_stage_models/v3_ppg2abp-kl-f4_2\\abp.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bsa\\AppData\\Local\\Temp\\ipykernel_34868\\1243015056.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pl_sd = torch.load(ckpt, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64) = 192 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "global step: 187108\n",
      "===========================================================================\n",
      "logging to:\n",
      "./models/first_stage_models/v3_ppg2abp-kl-f4_2\\samples\\00187108\\2024-12-06-01-07-47\n",
      "===========================================================================\n",
      "{'resume': './models/first_stage_models/v3_ppg2abp-kl-f4_2', 'n_samples': 50000, 'eta': 1.0, 'vanilla_sample': False, 'logdir': 'none', 'custom_steps': 50, 'batch_size': 10, 'base': ['./models/first_stage_models/v3_ppg2abp-kl-f4_2\\\\abp.yaml']}\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "sys.path.append(os.getcwd())\n",
    "command = \" \".join(sys.argv)\n",
    "\n",
    "parser = get_parser()\n",
    "opt, unknown = parser.parse_known_args()\n",
    "ckpt = None\n",
    "\n",
    "if not os.path.exists(opt.resume):\n",
    "    raise ValueError(\"Cannot find {}\".format(opt.resume))\n",
    "if os.path.isfile(opt.resume):\n",
    "    # paths = opt.resume.split(\"/\")\n",
    "    try:\n",
    "        logdir = '\\\\'.join(opt.resume.split('\\\\')[:-1])\n",
    "        # idx = len(paths)-paths[::-1].index(\"logs\")+1\n",
    "        print(f'Logdir is {logdir}')\n",
    "    except ValueError:\n",
    "        paths = opt.resume.split(\"\\\\\")\n",
    "        idx = -2  # take a guess: path/to/logdir/checkpoints/model.ckpt\n",
    "        logdir = \"\\\\\".join(paths[:idx])\n",
    "    ckpt = opt.resume\n",
    "else:\n",
    "    assert os.path.isdir(opt.resume), f\"{opt.resume} is not a directory\"\n",
    "    logdir = opt.resume.rstrip(\"\\\\\")\n",
    "    ckpt = os.path.join(logdir, \"abp.ckpt\")\n",
    "base_configs = sorted(glob.glob(os.path.join(logdir, \"abp.yaml\")))\n",
    "print(\"config\",base_configs)\n",
    "opt.base = base_configs\n",
    "\n",
    "configs = [OmegaConf.load(cfg) for cfg in opt.base]\n",
    "cli = OmegaConf.from_dotlist(unknown)\n",
    "config = OmegaConf.merge(*configs, cli)\n",
    "\n",
    "gpu = True\n",
    "eval_mode = True\n",
    "print(opt.logdir,logdir.split(os.sep),os.sep)\n",
    "if opt.logdir != \"none\":\n",
    "    locallog = logdir.split(os.sep)[-1]\n",
    "    print(locallog)\n",
    "    if locallog == \"\": locallog = logdir.split(os.sep)[-2]\n",
    "    print(f\"Switching logdir from '{logdir}' to '{os.path.join(opt.logdir, locallog)}'\")\n",
    "    logdir = os.path.join(opt.logdir, locallog)\n",
    "\n",
    "print(config)\n",
    "\n",
    "model, global_step = load_model(config, ckpt, gpu, eval_mode)\n",
    "print(f\"global step: {global_step}\")\n",
    "print(75 * \"=\")\n",
    "print(\"logging to:\")\n",
    "logdir = os.path.join(logdir, \"samples\", f\"{global_step:08}\", now)\n",
    "imglogdir = os.path.join(logdir, \"img\")\n",
    "numpylogdir = os.path.join(logdir, \"numpy\")\n",
    "\n",
    "os.makedirs(imglogdir)\n",
    "os.makedirs(numpylogdir)\n",
    "print(logdir)\n",
    "print(75 * \"=\")\n",
    "\n",
    "# write config out\n",
    "sampling_file = os.path.join(logdir, \"sampling_config.yaml\")\n",
    "sampling_conf = vars(opt)\n",
    "\n",
    "with open(sampling_file, 'w') as f:\n",
    "    yaml.dump(sampling_conf, f, default_flow_style=False)\n",
    "print(sampling_conf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Batches (conditional):   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data prepared: (5000, 256, 2)\n",
      "Running conditional sampling for 5000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Batches (conditional): 100%|█████████▉| 499/500 [00:05<00:00, 91.70it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish after generating 5000 samples\n",
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run(model, imglogdir, eta=opt.eta,\n",
    "vanilla=opt.vanilla_sample,  n_samples=opt.n_samples, custom_steps=opt.custom_steps,\n",
    "batch_size=opt.batch_size, nplog=numpylogdir)\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/CompVis/latent-diffusion/issues/187"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/diffusers/issues/437#issuecomment-1241827515"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "data prepared: (62528, 256, 2)\n",
      "normalizer = 0.42144223881543325\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "num_workers = 4\n",
    "batch_size = 12\n",
    "# From https://github.com/fastai/imagenette\n",
    "# IMAGENETTE_URL = 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz'\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# pretrained_model_name_or_path = 'CompVis/stable-diffusion-v1-4'\n",
    "# vae = AutoencoderKL.from_pretrained(\n",
    "#     pretrained_model_name_or_path,\n",
    "#     subfolder='vae',\n",
    "#     revision=None,\n",
    "# )\n",
    "vae = model\n",
    "vae.to(device)\n",
    "\n",
    "size = 256\n",
    "# image_transform = transforms.Compose([\n",
    "#     transforms.Resize(size),\n",
    "#     transforms.CenterCrop(size),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.5], [0.5]),\n",
    "# ])\n",
    "\n",
    "# root = 'dataset'\n",
    "# download_and_extract_archive(IMAGENETTE_URL, root)\n",
    "import ldm.data.ppg2abp as ppg2abp\n",
    "dataset = ppg2abp.PPG2ABPDataset_v3_Val(data_len=-1)\n",
    "n_samples = len(dataset)\n",
    "train_loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,shuffle=False)\n",
    "# dataset = torchvision.datasets.ImageFolder(root, transform=image_transform)\n",
    "# loader = torch.utils.data.DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True,\n",
    "#     num_workers=num_workers,\n",
    "# )\n",
    "\n",
    "all_latents = []\n",
    "for image_data in train_loader:\n",
    "    image_data = image_data[\"gt_image\"].to(device)\n",
    "    latents = vae.encode(image_data).sample()\n",
    "    all_latents.append(latents.cpu())\n",
    "\n",
    "all_latents_tensor = torch.cat(all_latents)\n",
    "std = all_latents_tensor.std().item()\n",
    "normalizer = 1 / std\n",
    "print(f'{normalizer = }')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38",
   "language": "python",
   "name": "venv38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
